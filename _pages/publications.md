---
layout: archive
title: ""
permalink: /publications/
author_profile: true
---

Preprints
----
5. Runze You\* and Shi Pu, [B-ary Tree Push-Pull Method is Provably Efficient for Decentralized Learning on Heterogeneous Data](https://arxiv.org/pdf/2404.05454.pdf), preprint.

4. Kun Huang\*, Shi Pu and Angelia Nedić, [An Accelerated Distributed Stochastic Gradient Method with Momentum](https://arxiv.org/pdf/2402.09714.pdf), submitted.

3. Kun Huang\*, Linli Zhou\* and Shi Pu, [Distributed Random Reshuffling Methods with Improved Convergence](https://arxiv.org/abs/2306.12037), submitted.

2. Kun Huang*, Xiao Li and Shi Pu, [Distributed Stochastic Optimization under a General Variance Condition](https://arxiv.org/abs/2301.12677), submitted.

1. Kun Huang* and Shi Pu, [CEDAS: A Compressed Decentralized Stochastic Gradient Method with Improved Convergence](https://arxiv.org/abs/2301.05872), submitted.

Journal Papers
----
16. Zhuoqing Song*, Lei Shi, Shi Pu and Ming Yan, [Provably Accelerated Decentralized Gradient Method Over Unbalanced Directed Graphs](https://arxiv.org/pdf/2107.12065.pdf), SIAM Journal on Optimization, accepted.

15. Zhuoqing Song*, Lei Shi, Shi Pu and Ming Yan, [Optimal Gradient Tracking for Decentralized Optimization](https://arxiv.org/pdf/2110.05282.pdf), Mathematical Programming, accepted.

14. Kun Huang*, Xiao Li, Andre Milzarek, Shi Pu and Junwen Qiu, [Distributed Random Reshuffling over Networks](https://arxiv.org/pdf/2112.15287.pdf), IEEE Transactions on Signal Processing, 71:1143-1158, 2023.

13. Yijie Zhou* and Shi Pu, [Private and Accurate Decentralized Optimization via Encrypted and Structured Functional Perturbation](https://arxiv.org/pdf/2209.01756.pdf), IEEE Control Systems Letters, 7:1339-1344, 2023.

12. Kun Huang* and Shi Pu, [Improving the Transient Times for Distributed Stochastic Gradient Methods](https://ieeexplore.ieee.org/document/9865230), IEEE Transactions on Automatic Control, 68(7):4127-4142, 2023.

11. Zhuoqing Song*, Lei Shi, Shi Pu and Ming Yan, [Compressed Gradient Tracking for Decentralized Optimization over General Directed Networks](https://ieeexplore.ieee.org/abstract/document/9737402), IEEE Transactions on Signal Processing, 70:1775-1787, 2022.

10. Yiwei Liao\*,  Zhuorui Li\*, Kun Huang* and Shi Pu, [A Compressed Gradient Tracking Method for Decentralized Optimization with Linear Convergence](https://ieeexplore.ieee.org/abstract/document/9789732), IEEE Transactions on Automatic Control, 67(10):5622-5629, 2022.

9. Shi Pu, Alex Olshevsky and Ioannis Ch. Paschalidis, [A Sharp Estimate on the Transient Time of Distributed Stochastic Gradient Descent](https://ieeexplore.ieee.org/abstract/document/9609587), IEEE Transactions on Automatic Control, 67(11):5900-5915, 2022.

8. Shi Pu and Angelia Nedić. [Distributed Stochastic Gradient Tracking Methods](https://link.springer.com/article/10.1007/s10107-020-01487-0). Mathematical Programming, 187(1):409-457, 2021. [[arXiv](https://arxiv.org/pdf/1805.11454.pdf)]

7. Shi Pu, Wei Shi (co-first), Jinming Xu and Angelia Nedić. [Push-Pull Gradient Methods for Distributed Optimization in Networks](https://ieeexplore.ieee.org/abstract/document/8988200). IEEE Transactions on Automatic Control, 66(1):1-16, 2021. [[arXiv](https://arxiv.org/pdf/1810.06653.pdf)]

6. Ran Xin, Shi Pu, Angelia Nedić and Usman Khan. [A General Framework for Decentralized Optimization with First-order Methods](https://ieeexplore.ieee.org/abstract/document/9241497). Proceedings of the IEEE, 108(11):1869-1889, 2020.

5. Shi Pu, Alex Olshevsky and Ioannis Ch. Paschalidis, [Asymptotic Network Independence in Distributed Stochastic Optimization for Machine Learning: Examining Distributed and Centralized Stochastic Gradient Descent](https://ieeexplore.ieee.org/abstract/document/9084351). IEEE Signal Processing Magazine, 37(3):114-122, 2020. [[arXiv](https://arxiv.org/pdf/1906.12345.pdf)].

4. Shi Pu, J. Joaquin Escudero-Garzás, Alfredo Garcia and Shahin Shahrampour. [An Online Mechanism for Resource Allocation in Networks](https://ieeexplore.ieee.org/abstract/document/8950126). IEEE Transactions on Control of Network Systems, 7(3):1140-1150, 2020.

3. Shi Pu and Alfredo Garcia. [Swarming for Faster Convergence in Stochastic Optimization](https://epubs.siam.org/doi/abs/10.1137/17M1111085). SIAM Journal on Control and Optimization, 56(4):2997-3020, 2018. [[arXiv](https://arxiv.org/pdf/1806.04207.pdf)]

2. Shi Pu and Alfredo Garcia. [A Flocking-based Approach for Distributed Stochastic Optimization](https://pubsonline.informs.org/doi/abs/10.1287/opre.2017.1666). Operations Research, 66(1):267-281, 2018. [[arXiv](https://arxiv.org/pdf/1709.07085.pdf)]

1. Shi Pu, Alfredo Garcia and Zongli Lin. [Noise Reduction by Swarming in Social Foraging](https://ieeexplore.ieee.org/abstract/document/7406677). IEEE Transactions on Automatic Control, 61(12):4007-4013, 2016.

Book Chapter
----
1. Alfredo Garcia, Bingyu Wang\* and Shi Pu. [Distributed Optimization](https://link.springer.com/referenceworkentry/10.1007/978-3-030-54621-2_809-1). In: Pardalos, P.M., Prokopyev, O.A. (eds) Encyclopedia of Optimization. Springer, Cham.

Conference Papers
----
4. Yiwei Liao*, Zhuorui Li and Shi Pu, [A Linearly Convergent Robust Compressed Push-Pull Method for Decentralized Optimization](https://arxiv.org/abs/2303.07091), 2023 IEEE 62th Conference on Decision and Control (CDC).

3. Shi Pu, [A Robust Gradient Tracking Method for Distributed Optimization over Directed Networks](https://ieeexplore.ieee.org/abstract/document/9303917), 2020 IEEE 59th Conference on Decision and Control (CDC).

2. Shi Pu and Angelia Nedić. A Distributed Stochastic Gradient Tracking Method. 2018 IEEE 57th Conference on Decision and Control (CDC).

1. Shi Pu, Wei Shi, Jinming Xu and Angelia Nedić. A Push-Pull Gradient Method for Distributed Optimization in Networks. 2018 IEEE 57th Conference on Decision and Control (CDC). 

***(co-)supervised student/postdoc**


{% if author.googlescholar %} You can also find my articles on my Google Scholar profile. {% endif %}

{% include base_path %}

{% for post in site.publications reversed %} {% include archive-single.html %} {% endfor %}
